{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadushb/Machine-Learning_Artificial-Neural-Network-ANN-/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRJu_U7-x8oC",
        "outputId": "8809b061-48e7-437e-c6c4-df1e1c142d91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 82 images belonging to 3 classes.\n",
            "Found 82 images belonging to 3 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 10s/step - accuracy: 0.3615 - loss: 1.3325 - val_accuracy: 0.6585 - val_loss: 0.8319\n",
            "Epoch 2/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.6604 - loss: 0.7831 - val_accuracy: 0.8659 - val_loss: 0.5217\n",
            "Epoch 3/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.8079 - loss: 0.5289 - val_accuracy: 0.8171 - val_loss: 0.4637\n",
            "Epoch 4/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.9249 - loss: 0.3440 - val_accuracy: 0.9146 - val_loss: 0.3206\n",
            "Epoch 5/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3s/step - accuracy: 0.8906 - loss: 0.3874 - val_accuracy: 0.9390 - val_loss: 0.2516\n",
            "Epoch 6/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3s/step - accuracy: 0.9221 - loss: 0.2923 - val_accuracy: 0.9390 - val_loss: 0.2041\n",
            "Epoch 7/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - accuracy: 0.9104 - loss: 0.2775 - val_accuracy: 0.9390 - val_loss: 0.2059\n",
            "Epoch 8/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3s/step - accuracy: 0.9532 - loss: 0.2025 - val_accuracy: 0.9512 - val_loss: 0.1592\n",
            "Epoch 9/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.9857 - loss: 0.1435 - val_accuracy: 0.9878 - val_loss: 0.1239\n",
            "Epoch 10/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.9965 - loss: 0.1102 - val_accuracy: 0.9878 - val_loss: 0.1125\n",
            "Epoch 11/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.9592 - loss: 0.1674 - val_accuracy: 0.9878 - val_loss: 0.1165\n",
            "Epoch 12/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3s/step - accuracy: 0.9965 - loss: 0.1207 - val_accuracy: 0.9878 - val_loss: 0.0959\n",
            "Epoch 13/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.9915 - loss: 0.1031 - val_accuracy: 0.9878 - val_loss: 0.0903\n",
            "Epoch 14/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.9806 - loss: 0.1187 - val_accuracy: 0.9756 - val_loss: 0.0862\n",
            "Epoch 15/15\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3s/step - accuracy: 0.9604 - loss: 0.1724 - val_accuracy: 0.9878 - val_loss: 0.0853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpuk_ne3l7'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  138468006794576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468006797648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468006803984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468006803600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468006802448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468006804176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468006802640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468006797072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468006804368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468006802832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468006804752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004119568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004119184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468006803024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004118992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004121296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004123216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004121872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004122256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004121104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004124752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004124368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004123984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004124944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004123600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004127056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004127440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004127824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004127632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004122448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004128976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004129360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004129744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004129552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004125904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004130896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004131280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004131664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004131472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004126672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004128592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004133392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004133776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004133584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004130128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004132816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983311504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983311888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004134544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138468004132432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983313040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983313424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983313808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983313616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983310928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983314960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983315344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983315728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983315536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983311312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983316880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983317264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983317648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983317456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983312656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983318800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983319184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983319568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983319376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983314576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983320720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983321104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983321488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983321296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983316496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983322640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983323024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983323408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983323216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983318416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983324560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983324944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983325328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983325136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983320336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983326480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983327056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983325712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983326864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983324176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983687952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983689296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983689680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983689488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983688144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983690832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983691216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983691600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983691408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983688720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983692752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983693136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983693520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983693328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983688912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983694672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983695056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983695440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983695248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983690448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983696592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983696976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983697360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983697168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983692368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983698512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983698896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983699280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983699088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983694288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983700432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983700816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983701200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983701008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983696208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983702352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983702736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983703120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983702928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983698128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983701584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982017168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982017552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983703888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467983700048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982018704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982019088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982019472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982019280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982016592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982020624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982021008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982021392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982021200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982016976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982022544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982022928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982023312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982023120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982018320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982024464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982024848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982025232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982025040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982020240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982026384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982026768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982027152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982026960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982022160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982028304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982028688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982029072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982028880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982024080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982030224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982030608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982030992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982030800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982026000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982032144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982032720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982031376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982032528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982029840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982344464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982345808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982346192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982346000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982344656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982347344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982347728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982348112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982347920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982345232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982349264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982349648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982350032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982349840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982345424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982351184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982351568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982351952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982351760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982346960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982353104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982353488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982353872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982353680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982348880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982355024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982355408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982355792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982355600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982350800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982356944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982357328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982357712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982357520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982352720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982358864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982359248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982359632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982359440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982354640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982358096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982787216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982786640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982360400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982356560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982788752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982789136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982789520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982789328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982786832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982790672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982791056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982791440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982791248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982787408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982792592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982792976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982793360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982793168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982788368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982794512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982794896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982795280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982795088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982790288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982796432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982796816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982797200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982797008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982792208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982798352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982798736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982799120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982798928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982794128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982800272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982800656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982801040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982800848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982796048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982802192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982802768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982801424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982802576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467982799888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467981214544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138467981216272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Model training and conversion complete!\n"
          ]
        }
      ],
      "source": [
        "# Step-by-step code to train and convert a facial recognition model for Raspberry Pi using TensorFlow\n",
        "# Assumes you have 3 folders: 'Alice', 'Bob', 'Carol' with images in 'dataset/train/' and 'dataset/val/'\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ========== Step 1: Define Constants ==========\n",
        "IMAGE_SIZE = (128, 128)\n",
        "BATCH_SIZE = 16\n",
        "NUM_CLASSES = 3  # Update this based on your dataset structure\n",
        "EPOCHS = 15\n",
        "\n",
        "# ========== Step 2: Prepare the Data ==========\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/dataset'\n",
        "val_dir = '/content/drive/MyDrive/Colab Notebooks/dataset'\n",
        "\n",
        "datagen_train = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    brightness_range=[0.7, 1.3],\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen_val = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = datagen_train.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "val_generator = datagen_val.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "# ========== Step 3: Build the Model ==========\n",
        "base_model = MobileNetV2(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False  # Freeze base\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "outputs = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ========== Step 4: Train the Model ==========\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# ========== Step 5: Save the Model ==========\n",
        "model.save(\"face_recognition_model.h5\")\n",
        "\n",
        "# ========== Step 6: Convert to TFLite ==========\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Use post-training quantization\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('face_recognition_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Model training and conversion complete!\")\n",
        "\n",
        "# ========== Step 7: Real-Time Model Monitoring Using TFLite ==========\n",
        "# Run this after training to test the TFLite model on webcam frames\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsb1Fzm91Xce"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=\"face_recognition_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "cap = cv2.VideoCapture(0)  # Use default camera\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    img = cv2.resize(frame, IMAGE_SIZE)\n",
        "    input_data = np.expand_dims(img.astype(np.float32) / 255.0, axis=0)\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "    interpreter.invoke()\n",
        "    predictions = interpreter.get_tensor(output_details[0]['index'])\n",
        "    class_id = np.argmax(predictions)\n",
        "    confidence = predictions[0][class_id]\n",
        "\n",
        "    label = \"{} ({:.2f}%)\".format(class_names[class_id], confidence * 100) if confidence > 0.6 else \"Unknown\"\n",
        "\n",
        "    cv2.putText(frame, label, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if label != \"Unknown\" else (0, 0, 255), 2)\n",
        "    cv2.imshow(\"Real-Time Face Recognition\", frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "cO9jr3nzraNN",
        "outputId": "ffa9ab99-d16b-47b0-a310-d888f4e7865d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 82 images belonging to 3 classes.\n",
            "TFLite model loaded successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video = document.createElement('video');\n",
              "    video.style.display = 'none';\n",
              "    document.body.appendChild(video);\n",
              "\n",
              "    var canvas = document.createElement('canvas');\n",
              "    canvas.style.display = 'none';\n",
              "    document.body.appendChild(canvas);\n",
              "\n",
              "    var context = canvas.getContext('2d');\n",
              "\n",
              "    var ul = document.createElement('ul');\n",
              "    document.body.appendChild(ul);\n",
              "\n",
              "    var stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "    video.srcObject = stream;\n",
              "    await video.play();\n",
              "\n",
              "    // Resize the canvas to match the video dimensions\n",
              "    canvas.width = video.videoWidth;\n",
              "    canvas.height = video.videoHeight;\n",
              "\n",
              "    var data = {};\n",
              "    data.toString = function() { return JSON.stringify(this); };\n",
              "\n",
              "    (async function loop() {\n",
              "      context.drawImage(video, 0, 0, video.videoWidth, video.videoHeight);\n",
              "      data.img = canvas.toDataURL('image/jpeg', 0.8); // Capture as JPEG\n",
              "\n",
              "      google.colab.output.enqueue(data);\n",
              "      await new Promise(requestAnimationFrame);\n",
              "      loop();\n",
              "    })();\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting live detection. Press the Stop button to end.\n",
            "Error during live detection: ReferenceError: data is not defined\n",
            "Live detection stopped.\n"
          ]
        }
      ],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab.patches import cv2_imshow # Import cv2_imshow\n",
        "\n",
        "# ========== Step 1: Define Constants ==========\n",
        "IMAGE_SIZE = (128, 128)\n",
        "NUM_CLASSES = 3  # Update this based on your dataset structure\n",
        "CONFIDENCE_THRESHOLD = 0.6 # Define a confidence threshold for detection\n",
        "\n",
        "# ========== Step 2: Prepare the Data (Needed for class_names) ==========\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/dataset'\n",
        "\n",
        "datagen_train = ImageDataGenerator(rescale=1./255) # Simple generator to get class names\n",
        "\n",
        "train_generator = datagen_train.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=1,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "# ========== Step 3: Load the TFLite Model ==========\n",
        "# Assuming you have already trained and saved your TFLite model\n",
        "# If not, you need to run the previous steps to generate \"face_recognition_model.tflite\"\n",
        "try:\n",
        "    interpreter = tf.lite.Interpreter(model_path=\"face_recognition_model.tflite\")\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    print(\"TFLite model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading TFLite model: {e}\")\n",
        "    print(\"Please ensure 'face_recognition_model.tflite' is in your Colab environment.\")\n",
        "    interpreter = None # Set interpreter to None if loading fails\n",
        "\n",
        "# ========== Step 4: JavaScript for Webcam Capture ==========\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video = document.createElement('video');\n",
        "    video.style.display = 'none';\n",
        "    document.body.appendChild(video);\n",
        "\n",
        "    var canvas = document.createElement('canvas');\n",
        "    canvas.style.display = 'none';\n",
        "    document.body.appendChild(canvas);\n",
        "\n",
        "    var context = canvas.getContext('2d');\n",
        "\n",
        "    var ul = document.createElement('ul');\n",
        "    document.body.appendChild(ul);\n",
        "\n",
        "    var stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "    video.srcObject = stream;\n",
        "    await video.play();\n",
        "\n",
        "    // Resize the canvas to match the video dimensions\n",
        "    canvas.width = video.videoWidth;\n",
        "    canvas.height = video.videoHeight;\n",
        "\n",
        "    var data = {};\n",
        "    data.toString = function() { return JSON.stringify(this); };\n",
        "\n",
        "    (async function loop() {\n",
        "      context.drawImage(video, 0, 0, video.videoWidth, video.videoHeight);\n",
        "      data.img = canvas.toDataURL('image/jpeg', 0.8); // Capture as JPEG\n",
        "\n",
        "      google.colab.output.enqueue(data);\n",
        "      await new Promise(requestAnimationFrame);\n",
        "      loop();\n",
        "    })();\n",
        "  ''')\n",
        "  display(js)\n",
        "\n",
        "# Function to decode the base64 image data\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Convert base64 image data to OpenCV image\n",
        "  \"\"\"\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  return img\n",
        "\n",
        "# Start the video stream (run this cell)\n",
        "video_stream()\n",
        "\n",
        "# ========== Step 5: Process Webcam Frames and Perform Inference ==========\n",
        "\n",
        "if interpreter is not None:\n",
        "    print(\"\\nStarting live detection. Press the Stop button to end.\")\n",
        "    try:\n",
        "        while True:\n",
        "            # Get the latest frame from the JavaScript stream\n",
        "            js_reply = eval_js('data')\n",
        "            frame = js_to_image(js_reply['img'])\n",
        "\n",
        "            # Resize frame to model input size\n",
        "            img_resized = cv2.resize(frame, IMAGE_SIZE)\n",
        "\n",
        "            # Prepare input tensor (adjust normalization if needed)\n",
        "            # If your model expects input in [0, 1], normalize like this:\n",
        "            # input_data = np.expand_dims(img_resized.astype(np.float32) / 255.0, axis=0)\n",
        "            # If your model expects input in [-1, 1], normalize like this:\n",
        "            input_data = np.expand_dims((img_resized.astype(np.float32) / 127.5) - 1.0, axis=0) # Example for [-1, 1]\n",
        "\n",
        "            # Run inference\n",
        "            interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "            interpreter.invoke()\n",
        "            predictions = interpreter.get_tensor(output_details[0]['index'])\n",
        "            class_id = np.argmax(predictions)\n",
        "            confidence = predictions[0][class_id]\n",
        "\n",
        "            predicted_label = \"Unknown\"\n",
        "            if confidence > CONFIDENCE_THRESHOLD:\n",
        "                predicted_label = f\"{class_names[class_id]} ({confidence:.2f}%)\"\n",
        "\n",
        "            # Draw the prediction on the frame\n",
        "            cv2.putText(frame, predicted_label, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if predicted_label != \"Unknown\" else (0, 0, 255), 2)\n",
        "\n",
        "            # Display the frame in Colab output\n",
        "            cv2_imshow(frame)\n",
        "\n",
        "            # Clear the output of the current display to avoid clutter\n",
        "            display(Javascript('google.colab.output.clear()'))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during live detection: {e}\")\n",
        "    finally:\n",
        "        print(\"Live detection stopped.\")\n",
        "else:\n",
        "    print(\"Cannot start live detection as the TFLite model failed to load.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "EVu89dFXsKN4",
        "outputId": "cc35a417-4ba7-46c2-81cf-b96381d8bf86"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 82 images belonging to 3 classes.\n",
            "TFLite model loaded successfully.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    var video = document.createElement('video');\n",
              "    video.style.display = 'none';\n",
              "    document.body.appendChild(video);\n",
              "\n",
              "    var canvas = document.createElement('canvas');\n",
              "    canvas.style.display = 'none';\n",
              "    document.body.appendChild(canvas);\n",
              "\n",
              "    var context = canvas.getContext('2d');\n",
              "\n",
              "    var ul = document.createElement('ul');\n",
              "    document.body.appendChild(ul);\n",
              "\n",
              "    var stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "    video.srcObject = stream;\n",
              "    await video.play();\n",
              "\n",
              "    // Resize the canvas to match the video dimensions\n",
              "    canvas.width = video.videoWidth;\n",
              "    canvas.height = video.videoHeight;\n",
              "\n",
              "    (async function loop() {\n",
              "      context.drawImage(video, 0, 0, video.videoWidth, video.videoHeight);\n",
              "      var img_data = canvas.toDataURL('image/jpeg', 0.8); // Capture as JPEG\n",
              "\n",
              "      // Call a Python function to process the frame\n",
              "      google.colab.kernel.invokeFunction('process_frame', [img_data], {});\n",
              "\n",
              "      await new Promise(requestAnimationFrame);\n",
              "      loop();\n",
              "    })();\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting live detection. Press the Stop button to end.\n"
          ]
        }
      ],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab.patches import cv2_imshow # Import cv2_imshow\n",
        "\n",
        "# ========== Step 1: Define Constants ==========\n",
        "IMAGE_SIZE = (128, 128)\n",
        "NUM_CLASSES = 3  # Update this based on your dataset structure\n",
        "CONFIDENCE_THRESHOLD = 0.6 # Define a confidence threshold for detection\n",
        "\n",
        "# ========== Step 2: Prepare the Data (Needed for class_names) ==========\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/dataset'\n",
        "\n",
        "datagen_train = ImageDataGenerator(rescale=1./255) # Simple generator to get class names\n",
        "\n",
        "train_generator = datagen_train.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=1,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "# ========== Step 3: Load the TFLite Model ==========\n",
        "try:\n",
        "    interpreter = tf.lite.Interpreter(model_path=\"face_recognition_model.tflite\")\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    print(\"TFLite model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading TFLite model: {e}\")\n",
        "    print(\"Please ensure 'face_recognition_model.tflite' is in your Colab environment.\")\n",
        "    interpreter = None # Set interpreter to None if loading fails\n",
        "\n",
        "# Global variable to store the latest frame\n",
        "latest_frame = None\n",
        "frame_ready = False\n",
        "\n",
        "# ========== Step 4: JavaScript for Webcam Capture ==========\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video = document.createElement('video');\n",
        "    video.style.display = 'none';\n",
        "    document.body.appendChild(video);\n",
        "\n",
        "    var canvas = document.createElement('canvas');\n",
        "    canvas.style.display = 'none';\n",
        "    document.body.appendChild(canvas);\n",
        "\n",
        "    var context = canvas.getContext('2d');\n",
        "\n",
        "    var ul = document.createElement('ul');\n",
        "    document.body.appendChild(ul);\n",
        "\n",
        "    var stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "    video.srcObject = stream;\n",
        "    await video.play();\n",
        "\n",
        "    // Resize the canvas to match the video dimensions\n",
        "    canvas.width = video.videoWidth;\n",
        "    canvas.height = video.videoHeight;\n",
        "\n",
        "    (async function loop() {\n",
        "      context.drawImage(video, 0, 0, video.videoWidth, video.videoHeight);\n",
        "      var img_data = canvas.toDataURL('image/jpeg', 0.8); // Capture as JPEG\n",
        "\n",
        "      // Call a Python function to process the frame\n",
        "      google.colab.kernel.invokeFunction('process_frame', [img_data], {});\n",
        "\n",
        "      await new Promise(requestAnimationFrame);\n",
        "      loop();\n",
        "    })();\n",
        "  ''')\n",
        "  display(js)\n",
        "\n",
        "# Function to decode the base64 image data and process it\n",
        "def process_frame(img_data_url):\n",
        "    global latest_frame, frame_ready\n",
        "    try:\n",
        "        frame = js_to_image(img_data_url)\n",
        "        latest_frame = frame\n",
        "        frame_ready = True\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing frame from JS: {e}\")\n",
        "\n",
        "# Function to decode the base64 image data\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Convert base64 image data to OpenCV image\n",
        "  \"\"\"\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  return img\n",
        "\n",
        "# Start the video stream (run this cell)\n",
        "video_stream()\n",
        "\n",
        "# Register the Python function to be called from JavaScript\n",
        "get_ipython().kernel.comm_manager.register_target('process_frame', process_frame)\n",
        "\n",
        "\n",
        "# ========== Step 5: Process Webcam Frames and Perform Inference ==========\n",
        "\n",
        "if interpreter is not None:\n",
        "    print(\"\\nStarting live detection. Press the Stop button to end.\")\n",
        "    try:\n",
        "        while True:\n",
        "            if frame_ready:\n",
        "                frame = latest_frame\n",
        "                frame_ready = False # Reset the flag\n",
        "\n",
        "                # Resize frame to model input size\n",
        "                img_resized = cv2.resize(frame, IMAGE_SIZE)\n",
        "\n",
        "                # Prepare input tensor (adjust normalization if needed)\n",
        "                # If your model expects input in [0, 1], normalize like this:\n",
        "                # input_data = np.expand_dims(img_resized.astype(np.float32) / 255.0, axis=0)\n",
        "                # If your model expects input in [-1, 1], normalize like this:\n",
        "                input_data = np.expand_dims((img_resized.astype(np.float32) / 127.5) - 1.0, axis=0) # Example for [-1, 1]\n",
        "\n",
        "                # Run inference\n",
        "                interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "                interpreter.invoke()\n",
        "                predictions = interpreter.get_tensor(output_details[0]['index'])\n",
        "                class_id = np.argmax(predictions)\n",
        "                confidence = predictions[0][class_id]\n",
        "\n",
        "                predicted_label = \"Unknown\"\n",
        "                if confidence > CONFIDENCE_THRESHOLD:\n",
        "                    predicted_label = f\"{class_names[class_id]} ({confidence:.2f}%)\"\n",
        "\n",
        "                # Draw the prediction on the frame\n",
        "                cv2.putText(frame, predicted_label, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if predicted_label != \"Unknown\" else (0, 0, 255), 2)\n",
        "\n",
        "                # Display the frame in Colab output\n",
        "                cv2_imshow(frame)\n",
        "\n",
        "                # Clear the output of the current display to avoid clutter\n",
        "                display(Javascript('google.colab.output.clear()'))\n",
        "            else:\n",
        "                # Wait a bit if no new frame is ready\n",
        "                import time\n",
        "                time.sleep(0.01)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during live detection: {e}\")\n",
        "    finally:\n",
        "        print(\"Live detection stopped.\")\n",
        "else:\n",
        "    print(\"Cannot start live detection as the TFLite model failed to load.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1zkTquwU5M4OjjxmyhJ2jHweGTdh9ToKZ",
      "authorship_tag": "ABX9TyO+yefqqKkxy7D6i0GZrHrR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}